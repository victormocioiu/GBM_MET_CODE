{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'Sunny'\n",
    "from collections import OrderedDict\n",
    "from itertools import izip, count\n",
    "import multiprocessing\n",
    "import os\n",
    "import cPickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold, LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from DataHolder import DataHolder\n",
    "from Processor import Proc_unit, wrapper\n",
    "\n",
    "\n",
    "def getData(conditions, condition_paths, echo):\n",
    "    \"\"\"Reads in memory all the data needed for the Glioblastoma vs Metastases paper.\n",
    "\n",
    "\tThis function could be used for any kind of pickled data that holds a dictionary.\n",
    "\tIt might not be the fastest function or the most pythonic, but I wrote it to get familiar with\n",
    "\tzip/izip - generators and the 'with' keyword.\n",
    "\n",
    "\tArgs:\n",
    "\t\tconditions: Iterable containing the conditions(String) to be investigated\n",
    "\t\tcondition_paths: Iterable containinf the paths to the conditions to be investigated\n",
    "\t\techo: String  ('LONG' or 'SHORT') corresponding to the TE of the acquisition\n",
    "\n",
    "\tReturns:\n",
    "\t\tdataset: Nested OrderedDict where every primary key is a patient. Every patient dict has the following struct-\n",
    "\t\tlabels : numpy 1D-array - the label (condition) for each patient\n",
    "\t\"\"\"\n",
    "    extension = 'pkl'\n",
    "    y = []\n",
    "    dataset = OrderedDict()\n",
    "    for index, condition, path in izip(count(), conditions, condition_paths):\n",
    "        with open(os.path.join(path, condition + '_' + echo + '.' + extension), 'rb')as condition_file:\n",
    "            data = pickle.load(condition_file)\n",
    "            y.append(np.zeros(len(data) + index))\n",
    "        dataset.update(data)\n",
    "    return dataset, np.hstack(y)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NR_SOURCES = 3\n",
    "    condit = [r'GBM', r'MET']\n",
    "#     dir  = os.path.dirname(__file__)\n",
    "\n",
    "    #paths = [os.path.join(dir,r'\\Data'), os.path.join(dir,r'\\Data')]# -DOES NOT WORK ON WINDOWS\n",
    "    paths = [r'C:\\\\Doctorat\\\\SOURCES\\\\GBM_MET_CODE\\Data' ,r'C:\\\\Doctorat\\\\SOURCES\\\\GBM_MET_CODE\\Data']\n",
    "    echos = 'LONG'\n",
    "    ## read in data\n",
    "    dataset, y = getData(condit, paths, echos)\n",
    "\n",
    "    ## classifiers to test out\n",
    "    clfs = [LogisticRegression(),\n",
    "        LogisticRegression(class_weight='auto'),\n",
    "        LogisticRegressionCV(),\n",
    "        LogisticRegressionCV(class_weight='auto'),\n",
    "        RandomForestClassifier(),\n",
    "        RandomForestClassifier(class_weight='auto'),\n",
    "        LDA(),\n",
    "        AdaBoostClassifier()\n",
    "       ]\n",
    "    ## funky format it\n",
    "    X = []\n",
    "    for patient in dataset.iterkeys():\n",
    "        X.append(patient)\n",
    "    X = np.array(X)\n",
    "    skf = LeaveOneOut(X.shape[0])\n",
    "    ##build max capacity arrays for holding train/test spectra\n",
    "    maxNr = 0\n",
    "    for patient in dataset.keys():\n",
    "        maxNr += dataset[patient]['Aligned'].shape[0]\n",
    "\n",
    "    ##make a list of DataHolder instances for each fold\n",
    "    folds = []\n",
    "    for train_index, test_index in skf:\n",
    "        ##1: split into training and testing set\n",
    "        spectra_train,spectra_test = np.zeros((maxNr,195)),np.zeros((maxNr,195))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        spectra_test_index = 0\n",
    "        spectra_train_index = 0\n",
    "        for index,patient in enumerate(dataset.keys()):\n",
    "            if patient in X_train:\n",
    "                spectra_train[spectra_train_index:spectra_train_index+dataset[patient]['Aligned'].shape[0],:] = dataset[patient]['Aligned']\n",
    "                spectra_train_index+=dataset[patient]['Aligned'].shape[0]\n",
    "            else:\n",
    "                spectra_test[spectra_test_index:spectra_test_index+dataset[patient]['Aligned'].shape[0],:] = dataset[patient]['Aligned']\n",
    "                spectra_test_index+=dataset[patient]['Aligned'].shape[0]\n",
    "                \n",
    "        spectra_train = spectra_train[:spectra_train_index,:]\n",
    "        spectra_test = spectra_test[:spectra_test_index,:]\n",
    "        folds.append(DataHolder(NR_SOURCES,dataset, X_train, X_test, y_train, y_test,\n",
    "                                spectra_train,spectra_test))\n",
    "    ## process in parallel\n",
    "    proc_units = [Proc_unit(data_holder,clfs) for data_holder in folds]\n",
    "\n",
    "\n",
    "    # no_of_process = multiprocessing.Pool(processes = max(1, multiprocessing.cpu_count()))\n",
    "    # results = no_of_process.map(wrapper,proc_units)\n",
    "    # no_of_process.close()\n",
    "    # no_of_process.join()\n",
    "    #\n",
    "    # pickle.dump(results,open('results.pkl','wb'))\n",
    "    # print \"Fuck you guys,I'm going home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proc_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = proc_units[:16]\n",
    "b = proc_units[16:32]\n",
    "c = proc_units[32:]\n",
    "\n",
    "filea = open('proc_a.pkl','wb')\n",
    "cPickle.dump(a,filea)\n",
    "filea.close()\n",
    "fileb = open('proc_b.pkl','wb')\n",
    "cPickle.dump(b,fileb)\n",
    "fileb.close()\n",
    "filec = open('proc_c.pkl','wb')\n",
    "cPickle.dump(c,filec)\n",
    "filec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
